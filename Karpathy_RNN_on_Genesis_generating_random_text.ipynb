{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "Karpathy RNN on Genesis -- generating random text.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drwitt/BME_590_Tensorflow_Deep_Learning/blob/master/Karpathy_RNN_on_Genesis_generating_random_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e46UywYM7Dt",
        "colab_type": "code",
        "outputId": "b65106d6-2847-4c6e-a16d-028b8aa25f21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''\n",
        "Updated in March 2018; Last updated in Aug 2019 by wz \n",
        "\n",
        "@author: A.Karpathy\n",
        "@editor: WZ\n",
        "'''\n",
        "\"\"\"\n",
        "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
        "BSD License\n",
        "\n",
        "\n",
        "You might need to run as well: \n",
        "\n",
        "import nltk\n",
        "nltk.download()\n",
        "\n",
        "to get the NLTK data \n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nMinimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\\nBSD License\\n\\n\\nYou might need also\\nimport nltk\\nnltk.download()\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT2NQ7DIOhhD",
        "colab_type": "text"
      },
      "source": [
        "Perhaps the easiest way to work with your data is to mount your google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN8N0FUQNIf-",
        "colab_type": "code",
        "outputId": "08162fd9-bcf1-40f8-8031-8b5678b4dd3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrPzr0vrOa4v",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOSg3oaeM7D5",
        "colab_type": "code",
        "outputId": "fd4e7e00-788f-416a-cca4-276f2f8739f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "inputtxt='/content/drive/My Drive/MIDS_Yr2/IDS_690_NLP/Text_Data/english-kjv.txt'\n",
        "\n",
        "print(inputtxt[0:500])\n",
        "# data I/O"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/MIDS_Yr2/IDS_690_NLP/Text_Data/english-kjv.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d0dW4L2M7D-",
        "colab_type": "code",
        "outputId": "905043f8-c096-4355-8020-d461c55743e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "data = open(inputtxt, 'r').read() # should be simple plain text file\n",
        "print(data[0:100])\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In the beginning God created the heaven and the earth.\n",
            "And the earth was without form, and void; and\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjIzjCkuQm3W",
        "colab_type": "code",
        "outputId": "36f5a19f-9b0a-4ef9-bf30-678038c560ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "chars = list(set(data))\n",
        "print('chars', chars)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chars ['h', 'b', 'e', 'i', 'a', 'R', 'W', 'F', 'J', 'w', 't', 's', 'N', 'H', '?', ';', 'B', 'Z', 'U', 'u', ' ', 'f', 'k', 'T', 'M', 'q', 'c', 'P', 'n', 'y', 'd', 'S', ':', 'I', 'v', 'Y', 'K', 'x', '\\n', 'g', 'r', '(', '!', 'l', ')', 'm', 'z', 'L', 'p', \"'\", 'E', 'C', 'G', 'j', 'o', 'A', 'D', 'O', ',', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h798PIwLM7EF",
        "colab_type": "code",
        "outputId": "4a1a0266-e124-42c0-95f6-510a1d49b974",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_size, vocab_size = len(data), len(chars)\n",
        "print ('data has %d characters, %d unique.' % (data_size, vocab_size))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data has 195515 characters, 60 unique.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWfgiCCWM7EL",
        "colab_type": "code",
        "outputId": "70cf59c9-6d1d-4f3b-defe-dc7cc520c874",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
        "print(char_to_ix)\n",
        "\n",
        "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
        "print(ix_to_char)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'h': 0, 'b': 1, 'e': 2, 'i': 3, 'a': 4, 'R': 5, 'W': 6, 'F': 7, 'J': 8, 'w': 9, 't': 10, 's': 11, 'N': 12, 'H': 13, '?': 14, ';': 15, 'B': 16, 'Z': 17, 'U': 18, 'u': 19, ' ': 20, 'f': 21, 'k': 22, 'T': 23, 'M': 24, 'q': 25, 'c': 26, 'P': 27, 'n': 28, 'y': 29, 'd': 30, 'S': 31, ':': 32, 'I': 33, 'v': 34, 'Y': 35, 'K': 36, 'x': 37, '\\n': 38, 'g': 39, 'r': 40, '(': 41, '!': 42, 'l': 43, ')': 44, 'm': 45, 'z': 46, 'L': 47, 'p': 48, \"'\": 49, 'E': 50, 'C': 51, 'G': 52, 'j': 53, 'o': 54, 'A': 55, 'D': 56, 'O': 57, ',': 58, '.': 59}\n",
            "{0: 'h', 1: 'b', 2: 'e', 3: 'i', 4: 'a', 5: 'R', 6: 'W', 7: 'F', 8: 'J', 9: 'w', 10: 't', 11: 's', 12: 'N', 13: 'H', 14: '?', 15: ';', 16: 'B', 17: 'Z', 18: 'U', 19: 'u', 20: ' ', 21: 'f', 22: 'k', 23: 'T', 24: 'M', 25: 'q', 26: 'c', 27: 'P', 28: 'n', 29: 'y', 30: 'd', 31: 'S', 32: ':', 33: 'I', 34: 'v', 35: 'Y', 36: 'K', 37: 'x', 38: '\\n', 39: 'g', 40: 'r', 41: '(', 42: '!', 43: 'l', 44: ')', 45: 'm', 46: 'z', 47: 'L', 48: 'p', 49: \"'\", 50: 'E', 51: 'C', 52: 'G', 53: 'j', 54: 'o', 55: 'A', 56: 'D', 57: 'O', 58: ',', 59: '.'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9H874JPM7ER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hyperparameters\n",
        "hidden_size = 100 # size of hidden layer of neurons\n",
        "seq_length = 25 # number of steps to unroll the RNN for\n",
        "learning_rate = 1e-1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEaK1rycM7EY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model parameters\n",
        "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
        "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
        "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
        "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
        "by = np.zeros((vocab_size, 1)) # output bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWrrn0XGM7Eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lossFun(inputs, targets, hprev):\n",
        "  \"\"\"\n",
        "  inputs,targets are both list of integers.\n",
        "  hprev is Hx1 array of initial hidden state\n",
        "  returns the loss, gradients on model parameters, and last hidden state\n",
        "  \"\"\"\n",
        "  xs, hs, ys, ps = {}, {}, {}, {}\n",
        "  hs[-1] = np.copy(hprev)\n",
        "  loss = 0\n",
        "  # forward pass\n",
        "  for t in range(len(inputs)):\n",
        "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
        "    xs[t][inputs[t]] = 1\n",
        "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
        "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
        "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
        "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
        "  # backward pass: compute gradients going backwards\n",
        "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
        "  dhnext = np.zeros_like(hs[0])\n",
        "  for t in reversed(range(len(inputs))):\n",
        "    dy = np.copy(ps[t])\n",
        "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
        "    dWhy += np.dot(dy, hs[t].T)\n",
        "    dby += dy\n",
        "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
        "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
        "    dbh += dhraw\n",
        "    dWxh += np.dot(dhraw, xs[t].T)\n",
        "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
        "    dhnext = np.dot(Whh.T, dhraw)\n",
        "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
        "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUr_oyKcM7En",
        "colab_type": "raw"
      },
      "source": [
        "\"\"\"   sample a sequence of integers from the model \n",
        "h is memory state, seed_ix is seed letter for first time step \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKvEzxYxM7Eq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(h, seed_ix, n):\n",
        "   x = np.zeros((vocab_size, 1))\n",
        "   x[seed_ix] = 1\n",
        "   ixes = []\n",
        "   for t in range(n):\n",
        "      h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
        "      y = np.dot(Why, h) + by\n",
        "      p = np.exp(y) / np.sum(np.exp(y))\n",
        "      ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
        "      x = np.zeros((vocab_size, 1))\n",
        "      x[ix] = 1\n",
        "      ixes.append(ix) \n",
        "   return ixes\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFWaRoJ4M7Ex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n, p = 0, 0\n",
        "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
        "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHftp12EWm2s",
        "colab_type": "text"
      },
      "source": [
        "### Please don't run it for 12h on colab \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ1E5i41M7E4",
        "colab_type": "code",
        "outputId": "5f10c350-d92b-4040-f1a3-ba47d2f2cc53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "while True:\n",
        "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
        "  if p+seq_length+1 >= len(data) or n == 0: \n",
        "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
        "    p = 0 # go from start of data\n",
        "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
        "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
        "\n",
        "  # sample from the model now and then\n",
        "  if n % 10000 == 0:  #was 1000\n",
        "    sample_ix = sample(hprev, inputs[0], 200)\n",
        "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
        "    print( '----\\n %s \\n----' % (txt, ))\n",
        "\n",
        "  # forward seq_length characters through the net and fetch gradient\n",
        "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
        "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "  if n % 10000 == 0: print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress #was 1000\n",
        "  \n",
        "  # perform parameter update with Adagrad\n",
        "  '''wz\n",
        "  Returns an iterator of tuples, where the i-th tuple contains the i-th element from each \n",
        "  of the argument sequences or iterables. \n",
        "  The iterator stops when the shortest input iterable is exhausted. \n",
        "  With a single iterable argument, it returns an iterator of 1-tuples.\n",
        "  '''\n",
        "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
        "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
        "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "    mem += dparam * dparam\n",
        "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
        "\n",
        "  p += seq_length # move data pointer\n",
        "  n += 1 # iteration counter "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----\n",
            " g;SG'jW:kYh gG?K)souTHZCbyI,:(fAPB'!CfTCJZE?!C.hsTD( (c',!; NeKTqre.BChieEiLMZNdpMGKC)m;,ZIPEpK Nr.pskaqtc:oapBLHxvtsdncyfYCki:CNCcPhwZ'Hyr!MmS?wGR.FkpAg?ahlHE;SEp'AT,r:)cU\n",
            "uIpifcd\n",
            "wtOeNhJi.n(ambqw.cz \n",
            "----\n",
            "iter 0, loss: 102.358618\n",
            "----\n",
            " rdild ficald lifirkid\n",
            "I hin?\n",
            "And Sind, and fasefrwo LORD\n",
            "him I fook shaut and upou said, and gyet kill with and, and to they sell\n",
            "wfounte fim, untors I hare dryered yif thal sofs of mad Ifreve, and Ab \n",
            "----\n",
            "iter 10000, loss: 44.658378\n",
            "----\n",
            " olded comcatiked called, the ry\n",
            "sank fith, the\n",
            "roughto berous theen, thing bed, fillant thou? the sard, God sperg the\n",
            "LORD shall hatt the fatll, by name him's wilen; brithd the gevered, what kard sped \n",
            "----\n",
            "iter 20000, loss: 40.071115\n",
            "----\n",
            " y Now the\n",
            "fither with be not be was\n",
            "be me mound their unto all Mimes eabret awe I heri\n",
            "Jame; be thing?\n",
            "Wefases years, The frund year unto were ea the wive is said\n",
            "ustine it uspined the\n",
            "Hableed\n",
            "cime an \n",
            "----\n",
            "iter 30000, loss: 38.915290\n",
            "----\n",
            " f waters of eacahe the drod wharath; ack and staraciface kin kind yown.\n",
            "And the arceruch the that\n",
            "which they of\n",
            "Abe hast and came up.\n",
            "And\n",
            "Us in the Lato, and in the vivens wave unto, and the eifst, ye \n",
            "----\n",
            "iter 40000, loss: 37.028103\n",
            "----\n",
            " ed up of the mounger an art log witeel, and he gave shir away of thou\n",
            "strount.\n",
            "And she mane, and fateltmln, And God sent him, which thy same fight my besh of thou a the plest.vep from the man to arter \n",
            "----\n",
            "iter 50000, loss: 36.861656\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-3abca79ec916>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'iter %d, loss: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print progress #was 1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-b6e72c13c965>\u001b[0m in \u001b[0;36mlossFun\u001b[0;34m(inputs, targets, hprev)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mdWxh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mdWhh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mdhnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdhraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mdparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdparam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# clip to mitigate exploding gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wsHUK1uM7FA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prior run sampled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9zgERPQM7FF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "----\n",
        " MlpSYgG)PscAJcCF)maYMxgcsAEMeaG'uG!awALhf\n",
        "nTbnBs'?C?EmdMe''JaLW)fDFH(zHGJdbkGhZNSP:pAWYvaYycGbpo,dx(iMZWUHK\n",
        "h)zLSqproOICzgvLbGo(.a, ?hK?v\n",
        "kDwLoWC;PiecyfmGnE:\n",
        "TLhj),gv\n",
        "F.k,s;\n",
        "cfL(IamKwqnNzihWusAFIROgIu \n",
        "----\n",
        "iter 0, loss: 102.358623\n",
        "----\n",
        " ll cworon of foed wa.\n",
        "c ide wtonAD fanty faJm aol dehibe ,he thee dald y,afas. hil fofe wahan.d; th aindeethet ans mnd eiri heter\n",
        "ind Nveyy\n",
        "falnle vathe thallt, url, of arert spy toat and tohy ge meeo \n",
        "----\n",
        "iter 1000, loss: 80.115990\n",
        "----\n",
        " , I unthedering alr thar iled Gof hing thevyrovend and herth ild, yam hes\n",
        "sorisu.\n",
        "pet w\n",
        "se\n",
        "kat his how le\n",
        "Ardixd I Hhile, ind\n",
        "the the\n",
        "mi goas bime Idind, And mald henling fomay, alaeg doter; wance I f \n",
        "----\n",
        "iter 2000, loss: 65.526007\n",
        "----\n",
        "  in noths of chou'd Gean pavenco gid liy the the he elrs thoins min ox.\n",
        "And the iod go sotoy saaven and ofhim he Noch the ahe on tee in ok thaecem, lhamthe salay.\n",
        "And sava htht thricarshers\n",
        "them thae  \n",
        "----\n",
        "iter 3000, loss: 57.927522\n",
        "----\n",
        " ---\n",
        "  wored all don terver,\n",
        "whot liverwe vift, wuund ban anten.\n",
        "And mad unto if us thy hes in thou unto her, and case shall not a thren\n",
        "sputh unto mazind him Abrahah.\n",
        "And they lill dounst will the king to  \n",
        "----\n",
        "iter 50000, loss: 42.217230\n",
        "----\n",
        " \n",
        "----\n",
        " \n",
        "And the sais\n",
        "Enou food men behold, spour him Hass in to hit fassy in theirs.\n",
        "And whous Phar' shau. And Joreh en the cond goed lose, and in on youre, and it tyough.f fore, and to sast\n",
        "ho, Behold be ao \n",
        "----\n",
        "iter 85000, loss: 40.487233\n",
        "----\n",
        "  king is my said, bucass. And he soed satild youl and the bros on Poweph yeom beforest land in shalssed; as at\n",
        "he notsy be Jacob I alle:\n",
        "Is\n",
        "ald.\n",
        "And rearftour: and they tles wild ald my falleld ha fol \n",
        "----\n",
        "iter 86000, loss: 39.939279\n",
        "----\n",
        "----\n",
        " may in\n",
        "Esaughid servaget Isaach the fapleb\n",
        "the begam, Unar theued and to the pare ase gad anoon bleires not sat at his sons\n",
        "wast sons thee pord hiveh burost sabob him a fie said, breons\n",
        "the eald shirk \n",
        "----\n",
        "iter 125000, loss: 39.021318\n",
        "----\n",
        " ed the mon of the earelvang, of will of God, and intt\n",
        "samal.\n",
        "And this hast they awale, Beho the comed douth, Pather spay madh they Leew, sseanter onding the lowtord terimese: Fearah of upon.\n",
        "The\n",
        "went  \n",
        "----\n",
        "iter 126000, loss: 38.998472\n",
        "----\n",
        "----\n",
        "iter 168000, loss: 38.126692\n",
        "----\n",
        " ee his not then, the seas tom shall said, I with silt, Os naml, when Abraht thy seevantsth, and houcher\n",
        "sthat caso is Abraham's nase teme. butobhes not head came of my\n",
        "are shailf begat kistrer, unto w \n",
        "----\n",
        "iter 169000, loss: 38.216707\n",
        "----\n",
        "pots, reart, and were agired \n",
        "----\n",
        "iter 199000, loss: 37.881385\n",
        "----\n",
        " hen\n",
        "unto the shall brouse,\n",
        "fill a lothered hid with me, dake will bearath all ane wiver is forth brock that eves,\n",
        "and shele upon\n",
        "the graceed vawat have Rachee inger as Adat in to Jacob be agained so o \n",
        "----\n",
        "iter 200000, loss: 37.349877\n",
        "----\n",
        " d his pin\n",
        "dreabinge out\n",
        "of Jadelis, and had up his said, whit I but thele of a feet this in to\n",
        "will of Hasron.\n",
        "And ithedd I dweldy mured's wi livedter he said, Whouseart shall Bechel\n",
        "in the in ano, an \n",
        "----\n",
        "----\n",
        "iter 218000, loss: 37.791892\n",
        "----\n",
        " hy agoin of the forchemebes Esaurn tham neraned hiigst years\n",
        "his owered at shall the brepet perdinge him\n",
        "Esiin remivering, wen, wheirn and the lersin unto the eard\n",
        "they up to thy giver tole offerd off \n",
        "----\n",
        "iter 219000, loss: 37.282680\n",
        "----\n",
        " 's of the king of hie they are\n",
        "not. Sorn me intt of thy cpae undremo.\n",
        "And the\n",
        "youprnas, bort unto\n",
        "the satelerp, and in that the ray, Learth.\n",
        "And the LORDarresh of hered is went LORD dreing madefe thou \n",
        "----\n",
        "iter 220000, loss: 37.477838\n",
        "----\n",
        " ilt said there\n",
        "of wad Abramaen dothy will lad ix the gilt with begai go dimied chanted, and give him upon the meish bach of thounstn his with my may ofly wifl it this sent, as no benceay.\n",
        "And they, an \n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}